"""
GPU ìµœì í™” ë‰´ìŠ¤ ìš”ì•½ ëª¨ë¸ ì˜ˆì¸¡ê¸° 
RTX 2080 + bitsandbytes 4bit ì–‘ìí™” ì§€ì›
"""
import os
import gc
import torch
import logging
from typing import Optional
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import PeftModel

logger = logging.getLogger(__name__)

class SummarizerPredictor:
    """GPU ìµœì í™” ìš”ì•½ ëª¨ë¸ ì˜ˆì¸¡ê¸° (RTX 2080 ì „ìš©)"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        
        # GPU ê°•ì œ ì„¤ì • (RTX 2080)
        if not torch.cuda.is_available():
            raise RuntimeError("âŒ CUDA GPUê°€ í•„ìš”í•©ë‹ˆë‹¤. RTX 2080ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.device = "cuda:0"
        torch.cuda.set_device(0)  # RTX 2080 ì„ íƒ
        
        self.base_model_name = "skt/kogpt2-base-v2"  # KoGPT2 í•œêµ­ì–´ ìƒì„± ëª¨ë¸
        
        # í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ê²½ë¡œ í™•ì¸
        trained_adapter_path = "/app/slm_summarizer_training/outputs"
        if os.path.exists(trained_adapter_path) and os.path.exists(os.path.join(trained_adapter_path, "adapter_config.json")):
            self.model_path = trained_adapter_path
            logger.info(f"ğŸ“ í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ë°œê²¬: {trained_adapter_path}")
        else:
            self.model_path = "./outputs"  # í´ë°± ê²½ë¡œ
            logger.warning(f"âš ï¸ í•™ìŠµëœ ì–´ëŒ‘í„° ì—†ìŒ, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©")
        
        logger.info(f"ğŸš€ GPU í™˜ê²½ ì´ˆê¸°í™” ì™„ë£Œ: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
    async def load_model(self):
        """RTX 2080 GPU ìµœì í™” ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info("ğŸ”„ GPU ê¸°ë°˜ KoGPT2 ëª¨ë¸ ë¡œë”© ì‹œì‘...")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            gc.collect()
            
            # RTX 2080 ìµœì í™” 4bit ì–‘ìí™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_storage=torch.uint8,
                # RTX 2080 ìµœì í™”
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False
            )
            
            # KoGPT2 í† í¬ë‚˜ì´ì € ë¡œë”© (ì˜¤í”„ë¼ì¸ ìš°ì„ )
            logger.info("ğŸ“– KoGPT2 í† í¬ë‚˜ì´ì € ë¡œë”©...")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.base_model_name,
                    trust_remote_code=True,
                    use_fast=False,
                    local_files_only=False,  # HuggingFace Hub í—ˆìš©
                    token=os.getenv("HUGGINGFACE_HUB_TOKEN")  # API ì œí•œ ìš°íšŒ
                )
                
                # KoGPT2 í•„ìˆ˜ ì„¤ì •
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
                
                self.tokenizer.padding_side = "left"
                self.tokenizer.model_max_length = 512
                
                logger.info(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ (vocab: {self.tokenizer.vocab_size})")
                
            except Exception as e:
                logger.error(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
            
            # RTX 2080 ìµœì í™” ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© 
            logger.info("ğŸ¤– KoGPT2 ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© (GPU + 4bit)...")
            try:
                base_model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    quantization_config=bnb_config,
                    device_map="auto",  # RTX 2080ì— ìë™ ë°°ì¹˜
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                    max_memory={0: "7GB"},  # RTX 2080 8GB ì¤‘ 7GB ì‚¬ìš©
                    use_cache=True,
                    attn_implementation="eager",  # RTX 2080 í˜¸í™˜
                    token=os.getenv("HUGGINGFACE_HUB_TOKEN")
                )
                
                logger.info(f"âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                # í´ë°±: ë” ë³´ìˆ˜ì ì¸ ì„¤ì •
                logger.info("ğŸ”„ ë³´ìˆ˜ì  ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...")
                base_model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    quantization_config=bnb_config,
                    device_map={"": 0},  # ê°•ì œë¡œ GPU 0ì— ë°°ì¹˜
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                    token=os.getenv("HUGGINGFACE_HUB_TOKEN")
                )
            
            # LoRA ì–´ëŒ‘í„° í™•ì¸ ë° ë¡œë”©
            if os.path.exists(self.model_path) and os.path.exists(os.path.join(self.model_path, "adapter_config.json")):
                logger.info(f"ğŸ”— LoRA ì–´ëŒ‘í„° ë¡œë”©: {self.model_path}")
                self.model = PeftModel.from_pretrained(base_model, self.model_path)
            else:
                logger.warning(f"âš ï¸ LoRA ì–´ëŒ‘í„° ì—†ìŒ, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©: {self.model_path}")
                self.model = base_model
            
            # ì¶”ë¡  ëª¨ë“œ ì„¤ì •
            self.model.eval()
            if hasattr(self.model, 'gradient_checkpointing_disable'):
                self.model.gradient_checkpointing_disable()
            
            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            memory_allocated = torch.cuda.memory_allocated(0) / 1e9
            memory_reserved = torch.cuda.memory_reserved(0) / 1e9
            logger.info(f"ğŸ¯ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
            
            logger.info("ğŸ‰ GPU ìµœì í™” KoGPT2 ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ GPU ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise e
    
    async def generate_summary(
        self,
        title: str,
        description: str,
        max_new_tokens: int = 100,  # RTX 2080 ë©”ëª¨ë¦¬ ê³ ë ¤
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """GPU ìµœì í™” ìš”ì•½ ìƒì„±"""
        try:
            if self.model is None or self.tokenizer is None:
                raise ValueError("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._create_prompt(title, description)
            
            # GPU í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=400,  # ì…ë ¥ ê¸¸ì´ ì œí•œ (RTX 2080 ìµœì í™”)
                padding=True
            ).to(self.device)
            
            # GPU ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=3,
                    early_stopping=True,
                    use_cache=True
                )
            
            # ê²°ê³¼ ë””ì½”ë”©
            generated_text = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            # ì •ë¦¬
            summary = self._clean_generated_text(generated_text)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            del inputs, outputs
            torch.cuda.empty_cache()
            
            return summary if summary else "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            logger.error(f"âŒ GPU ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            torch.cuda.empty_cache()  # ì—ëŸ¬ ì‹œì—ë„ ë©”ëª¨ë¦¬ ì •ë¦¬
            return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def _create_prompt(self, title: str, description: str) -> str:
        """KoGPT2 ìµœì í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë‚´ìš©: {description}

ìš”ì•½:"""

    def _clean_generated_text(self, text: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        cleaned = text.split('\n')[0].strip()
        cleaned = cleaned.replace('ìš”ì•½:', '').strip()
        
        # ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ ìš”ì•½ ë°©ì§€)
        if len(cleaned) > 200:
            cleaned = cleaned[:200] + "..."
        
        return cleaned
    
    def unload_model(self):
        """GPU ë©”ëª¨ë¦¬ì—ì„œ ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.tokenizer is not None:
            del self.tokenizer  
            self.tokenizer = None
        
        torch.cuda.empty_cache()
        gc.collect()
        logger.info("ğŸ—‘ï¸ GPU ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
    
    def get_gpu_memory_info(self) -> dict:
        """GPU ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        if torch.cuda.is_available():
            return {
                "allocated": torch.cuda.memory_allocated(0) / 1e9,
                "reserved": torch.cuda.memory_reserved(0) / 1e9,
                "total": torch.cuda.get_device_properties(0).total_memory / 1e9
            }
        return {}
    
    @property
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
        return self.model is not None and self.tokenizer is not None 