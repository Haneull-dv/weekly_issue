from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from typing import Dict, Any, Optional
from datetime import datetime
import logging

# ê³µí†µ DB ëª¨ë“ˆ import
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))
from weekly_db.db.db_builder import get_db_session
from weekly_db.db.weekly_service import WeeklyDataService, WeeklyBatchService
from weekly_db.db.weekly_unified_model import WeeklyDataModel

# Issue ì„œë¹„ìŠ¤ import
from app.domain.controller.issue_controller import IssueController
from app.config.companies import COMPANY_NAMES, GAME_COMPANIES

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/n8n")

@router.post("/collect-issues")
async def collect_issues_for_n8n(
    week: Optional[str] = None,
    db: AsyncSession = Depends(get_db_session)
) -> Dict[str, Any]:
    """
    ğŸ¤– n8n ìë™í™”: ì „ì²´ ê²Œì„ê¸°ì—… ì´ìŠˆ ë¶„ì„ ìˆ˜ì§‘
    
    ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 7ì‹œì— n8nì´ ìë™ í˜¸ì¶œ
    configì— ë“±ë¡ëœ ëª¨ë“  ê²Œì„ê¸°ì—…ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì—¬ AI ë¶„ì„ í›„ weekly_data í…Œì´ë¸”ì— ëˆ„ì  ì €ì¥
    
    ì²˜ë¦¬ ê³¼ì •:
    1. ê° ê¸°ì—…ë‹¹ 100ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘
    2. í‚¤ì›Œë“œ í•„í„°ë§
    3. AI ë¶„ë¥˜ (ì¤‘ìš”ë„ íŒë³„)
    4. AI ìš”ì•½ ìƒì„±
    5. DB ì €ì¥
    
    Args:
        week: ëŒ€ìƒ ì£¼ì°¨ (YYYY-MM-DD, Noneì´ë©´ í˜„ì¬ ì£¼)
    
    Returns:
        {"status": "success", "updated": 8, "skipped": 3, "week": "2025-01-13"}
    """
    
    if not week:
        week = WeeklyDataModel.get_current_week_monday()
    
    logger.info(f"ğŸ¤– n8n ì´ìŠˆ ë¶„ì„ ì‹œì‘ - Week: {week}, Companies: {len(COMPANY_NAMES)}")
    
    # ë°°ì¹˜ ì‘ì—… ì‹œì‘ ë¡œê·¸
    batch_service = WeeklyBatchService(db)
    job_id = await batch_service.start_batch_job("issue", week)
    
    try:
        # 1. ê¸°ì¡´ Issue Controllerë¡œ ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ (ëª¨ë“  ê¸°ì—…)
        controller = IssueController(db_session=db)
        
        logger.info(f"ğŸ” ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹œì‘ - {len(COMPANY_NAMES)}ê°œ ê¸°ì—…")
        # companies=Noneì´ë©´ ìë™ìœ¼ë¡œ ëª¨ë“  ê¸°ì—… ì²˜ë¦¬
        pipeline_result = await controller.process_news_pipeline(None)
        
        logger.info(f"ğŸ“‹ ì´ìŠˆ ë¶„ì„ ì™„ë£Œ - {len(pipeline_result.results)}ê±´")
        
        # 2. weekly_data í…Œì´ë¸”ìš© ë°ì´í„° ë³€í™˜
        weekly_items = []
        for issue in pipeline_result.results:
            # ê¸°ì—…ëª…ìœ¼ë¡œ ì¢…ëª©ì½”ë“œ ì°¾ê¸° (ì—­ë§¤í•‘)
            stock_code = None
            for code, name in GAME_COMPANIES.items():
                if name == issue.corp:
                    stock_code = code
                    break
            
            weekly_item = {
                "company_name": issue.corp,
                "content": issue.summary,  # AI ìš”ì•½ ë‚´ìš©ì„ ë©”ì¸ ì»¨í…ì¸ ë¡œ
                "stock_code": stock_code,
                "metadata": {
                    "original_title": issue.original_title,
                    "confidence": issue.confidence,
                    "matched_keywords": issue.matched_keywords,
                    "news_url": getattr(issue, 'news_url', None),
                    "published_date": getattr(issue, 'published_date', None),
                    "category": getattr(issue, 'category', None),
                    "sentiment": getattr(issue, 'sentiment', None),
                    "source": "naver_news_api"
                }
            }
            weekly_items.append(weekly_item)
        
        # 3. WeeklyDataServiceë¡œ í†µí•© í…Œì´ë¸”ì— ì €ì¥
        weekly_service = WeeklyDataService(db)
        result = await weekly_service.bulk_upsert_weekly_data(
            weekly_items=weekly_items,
            category="issue",
            week=week
        )
        
        # 4. ë°°ì¹˜ ì‘ì—… ì™„ë£Œ ë¡œê·¸
        await batch_service.finish_batch_job(job_id, result)
        
        logger.info(f"âœ… n8n ì´ìŠˆ ë¶„ì„ ì™„ë£Œ - {result}")
        
        return {
            "status": result["status"],
            "updated": result["updated"],
            "skipped": result["skipped"],
            "errors": result["errors"],
            "week": result["week"],
            "total_companies": len(COMPANY_NAMES),
            "pipeline_stats": {
                "total_collected": pipeline_result.total_collected,
                "after_keyword_filter": pipeline_result.after_keyword_filter,
                "after_classification": pipeline_result.after_classification,
                "final_summaries": pipeline_result.final_summaries
            },
            "job_id": job_id
        }
        
    except Exception as e:
        logger.error(f"âŒ n8n ì´ìŠˆ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        # ë°°ì¹˜ ì‘ì—… ì‹¤íŒ¨ ë¡œê·¸
        await batch_service.finish_batch_job(job_id, {}, str(e))
        
        raise HTTPException(
            status_code=500, 
            detail=f"ì´ìŠˆ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


@router.get("/issues/status")
async def get_issues_collection_status(
    week: Optional[str] = None,
    db: AsyncSession = Depends(get_db_session)
) -> Dict[str, Any]:
    """
    ğŸ“Š ì´ìŠˆ ë¶„ì„ ìƒíƒœ ì¡°íšŒ
    
    íŠ¹ì • ì£¼ì°¨ì˜ ì´ìŠˆ ë°ì´í„° ìˆ˜ì§‘ í˜„í™©ì„ ë°˜í™˜
    """
    if not week:
        week = WeeklyDataModel.get_current_week_monday()
    
    try:
        weekly_service = WeeklyDataService(db)
        
        # í•´ë‹¹ ì£¼ì°¨ ì´ìŠˆ ë°ì´í„° ì¡°íšŒ
        issue_data = await weekly_service.get_weekly_data(
            week=week, 
            category="issue"
        )
        
        # ë°°ì¹˜ ì‘ì—… ë¡œê·¸ ì¡°íšŒ
        batch_service = WeeklyBatchService(db)
        recent_jobs = await batch_service.get_recent_jobs(job_type="issue", limit=5)
        
        # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
        confidences = []
        for item in issue_data:
            if item.get("metadata", {}).get("confidence"):
                confidences.append(item["metadata"]["confidence"])
        
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        return {
            "week": week,
            "issue_count": len(issue_data),
            "companies_collected": len(set(item["company_name"] for item in issue_data)),
            "total_target_companies": len(COMPANY_NAMES),
            "avg_confidence": round(avg_confidence, 3),
            "high_confidence_count": len([c for c in confidences if c >= 0.8]),
            "recent_jobs": recent_jobs,
            "sample_data": issue_data[:3] if issue_data else []
        }
        
    except Exception as e:
        logger.error(f"âŒ ì´ìŠˆ ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì´ìŠˆ ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )


@router.get("/issues/weeks")
async def get_available_issue_weeks(
    limit: int = 10,
    db: AsyncSession = Depends(get_db_session)
) -> Dict[str, Any]:
    """ğŸ“… ì´ìŠˆ ë°ì´í„°ê°€ ìˆëŠ” ì£¼ì°¨ ëª©ë¡ ì¡°íšŒ"""
    try:
        weekly_service = WeeklyDataService(db)
        weeks = await weekly_service.get_available_weeks(limit=limit)
        
        return {
            "available_weeks": weeks,
            "total_weeks": len(weeks),
            "latest_week": weeks[0] if weeks else None
        }
        
    except Exception as e:
        logger.error(f"âŒ ì£¼ì°¨ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì£¼ì°¨ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        ) 